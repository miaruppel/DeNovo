{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c75257c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pyopenms import * # main package used for handling MS data\n",
    "import os # changing directories\n",
    "import pandas as pd # creating and manipulating dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42ae3074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to find data files of interest\n",
    "os.chdir(r'C:\\Users\\miar\\Desktop\\data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4bd296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment files \n",
    "mzML = 'HEK293T_De_Novo_061122_Glu-C_B_BP_anyLength_HCD10.mzML'\n",
    "log = 'App-2022-06-12_14-16-26.log'\n",
    "realtime = 'HEK293T_De_Novo_061122_Glu-C_B_BP_anyLength_HCD10_realtimesearch.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cec58c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the content of the mzML file into the exp variable of type MSExperiment\n",
    "exp = MSExperiment()\n",
    "MzMLFile().load(mzML, exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85506994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the spectra to filter MS3 scans\n",
    "specM3 = [] # list of MS3 spectra \n",
    "row_data = []\n",
    "for s in exp.getSpectra():\n",
    "    \n",
    "    if s.getMSLevel() == 3:\n",
    "        specM3.append(s)\n",
    "        \n",
    "        # get scan number\n",
    "        s_number = s.getNativeID().split(' ')[-1]\n",
    "        _, scan_number = s_number.split('=')\n",
    "        \n",
    "        # obtain mz and intensity values \n",
    "        mz, intensity = s.get_peaks()\n",
    "        \n",
    "        mz_mod = \" \".join(str(m) for m in mz)\n",
    "        intensity_mod = \" \".join(str(i) for i in intensity)\n",
    "        \n",
    "        # create dict (rows of dataframe)\n",
    "        data = {'MS3_Scan':scan_number,\n",
    "       'masses_raw':mz_mod,\n",
    "       'intensities_raw':intensity_mod}\n",
    "        \n",
    "        row_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a6e1d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set MS3 scans as index\n",
    "df = pd.DataFrame(row_data)\n",
    "df.set_index('MS3_Scan', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43d94390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse functions\n",
    "def parseScanLine(input):\n",
    "    x = input.split(\" For: \")\n",
    "    [scan_number, mzs] = x[1].split(\", \")\n",
    "    [precursor_mz, fragment_mz] = mzs.split(\";\")\n",
    "    trimmed_fragment_mz = fragment_mz.strip() # trim fragment strings to remove \\n\n",
    "    return [scan_number, precursor_mz, trimmed_fragment_mz]\n",
    "\n",
    "def parseTargetIons(input):\n",
    "    i = input.split('Target Fragment: ')\n",
    "    ion = i[1].split(',')[0]\n",
    "    return ion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "409aaf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking lines of log file and creating dictionary of scan numbers and fragment mzs\n",
    "try:\n",
    "  \n",
    "    # words to search for\n",
    "    search = ' Submitted Custom Scan For:'\n",
    "    \n",
    "    # dict for scan numbers and corresponding fragments \n",
    "    scan2frag = dict()\n",
    "    with open(log) as f:\n",
    "        for line in f:\n",
    "            if search in line:\n",
    "                scan_number, precursor_mz, trimmed_fragment_mz = parseScanLine(line)\n",
    "                scan2frag[scan_number] = [float(precursor_mz), float(trimmed_fragment_mz)]\n",
    "            \n",
    "    # if the input string doesn't exist in the text file\n",
    "    if len(scan2frag)==0:\n",
    "        print(\"\\n\\\"\" + search + \"\\\" is not found in \\\"\" + log + \"\\\"!\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"The file does not exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57d3272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain MS3 scan numbers\n",
    "# obtain precursor and fragment mzs directly from the MS3 spectrum\n",
    "\n",
    "ms3scan2MZs = dict()\n",
    "for s in specM3:\n",
    "    s_number = s.getNativeID().split(' ')[-1]\n",
    "    _, scan_number = s_number.split('=')\n",
    "   \n",
    "    fragment, precursor = s.getPrecursors()\n",
    "    precursor_mz = precursor.getMZ()\n",
    "    fragment_mz = fragment.getMZ()\n",
    "    \n",
    "    ms3scan2MZs[int(scan_number)] = [round(float(precursor_mz), 4), round(float(fragment_mz), 4)] # 4 decimal places, similar to log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8932c983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchingMS3s(ms2_mzs, ms3_mzs): # either fragment or precursor\n",
    "    \n",
    "    # making sure they are within 100 scans of each other\n",
    "    too_far = []\n",
    "    for ms2scan, ms3scan in zip(list(scan2frag), list(ms3scan2MZs)):\n",
    "        scan_diff = int(ms3scan) - int(ms2scan)\n",
    "        if scan_diff > 100:\n",
    "            too_far.append('Scans are not within 100 scans of each other...' + 'MS2 = ' + str(ms2scan) + ' MS3 = ' + str(ms3scan))\n",
    "    \n",
    "    # do they not match off the bat?\n",
    "    if ms2_mzs != ms3_mzs:\n",
    "        # taking into consideration rounding discrepencies between the log and the spectrum\n",
    "        mismatch = []\n",
    "        for i in range(0, len(list(ms3scan2MZs))):\n",
    "            \n",
    "            precursor_diff = float(list(ms3scan2MZs.values())[i][0]) - float(list(scan2frag.values())[i][0]) \n",
    "            if precursor_diff < 0.000101 or (precursor_diff < 0 and precursor_diff > -0.000101): # because sometimes max number will be 0.0001000002 for example\n",
    "                pass\n",
    "            else:\n",
    "                mismatch.append(i)\n",
    "            \n",
    "            fragment_diff = float(list(ms3scan2MZs.values())[i][1]) - float(list(scan2frag.values())[i][1]) \n",
    "            if fragment_diff < 0.000101 or (fragment_diff < 0 and fragment_diff > -0.000101): # because sometimes max number will be 0.0001000002 for example\n",
    "                pass\n",
    "            else:\n",
    "                mismatch.append(i)\n",
    "                \n",
    "        # no mismatch after rounding and within 100 scans\n",
    "        if len(mismatch) == len(too_far) == 0:\n",
    "            print('Scans match up after taking rounding discrepencies into consideration')\n",
    "            ms2_scans = list(scan2frag)\n",
    "            return ms2_scans\n",
    "        \n",
    "        elif len(mismatch) != 0:\n",
    "            print('There is mismatch at the following indicies:') # if this is the case, need to do more work...\n",
    "            for i in mismatch:\n",
    "                print(i) \n",
    "                \n",
    "        elif len(too_far) != 0:\n",
    "            print(too_far)\n",
    "        \n",
    "    # they match perfectly\n",
    "    elif ms2_mzs == ms3_mzs:\n",
    "        # within 100 scans\n",
    "        if len(too_far) == 0:\n",
    "            print('Scans match up perfectly!')\n",
    "            ms2_scans = list(scan2frag)\n",
    "            return ms2_scans\n",
    "        else:\n",
    "            print(too_far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "230d2e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scans match up after taking rounding discrepencies into consideration\n"
     ]
    }
   ],
   "source": [
    "# make sure that MS3 scans are in the same order as MS2 scans\n",
    "ms2_scans = matchingMS3s(list(ms3scan2MZs.values()), list(scan2frag.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95d0074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use realtime file to obtain peptide sequence and charge\n",
    "# read in peptide sequence from tsv\n",
    "tsv = pd.read_csv(realtime, sep='\\t')\n",
    "\n",
    "# create dictionary with scan # as key and sequence/charge as values\n",
    "scan2PeptideCharge = dict([(i, [x,y]) for i, x,y, in zip(tsv['Scan Number'], tsv['Peptide'], tsv['Charge State'])])\n",
    "\n",
    "# removing all NaN sequences (not useful)\n",
    "scan2PeptideCharge_modified = {k:v for k,v in scan2PeptideCharge.items() if str(v[0]) != 'nan'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8ed0a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect data for dataframe\n",
    "seqs = []\n",
    "charges = []\n",
    "analyzer = []\n",
    "collision = []\n",
    "\n",
    "energy = int(realtime.split('_')[-2][-2:])\n",
    "\n",
    "for scan in ms2_scans:\n",
    "    if int(scan) in list(scan2PeptideCharge_modified):\n",
    "        charge = scan2PeptideCharge_modified[int(scan)][1]\n",
    "        charges.append(charge)\n",
    "        \n",
    "        sequence = scan2PeptideCharge_modified[int(scan)][0]     \n",
    "        trimmed_sequence = sequence[2:-2] # remove first two and last two characters \n",
    "        seqs.append(trimmed_sequence)\n",
    "        \n",
    "        # all ms3 scans are orbit trap \n",
    "        # to be added to dataframe with other MS3 info\n",
    "        analyzer.append('FTMS')\n",
    "        \n",
    "        # all scans have same collision energy\n",
    "        collision.append(energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "022c4593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all data columns to dataframe\n",
    "df = df.assign(charge=charges, modified_sequence=seqs, mass_analyzer=analyzer, collision_energy=collision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02d58968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all modified sequences\n",
    "for i in df.index:\n",
    "    if ('[' or ']') in df['modified_sequence'][i]:\n",
    "        df.drop(i, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7960147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing variables and functions from other scripts\n",
    "from constants import ION_TYPES, DEFAULT_MAX_CHARGE\n",
    "from match import augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c4c7544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running augment function\n",
    "df_augmented = augment(df, ION_TYPES, DEFAULT_MAX_CHARGE)\n",
    "df_augmented.sort_values(by='matches_charge1', inplace=True)\n",
    "\n",
    "# remove all rows that are completely empty\n",
    "for i in df_augmented.index:\n",
    "    if df_augmented['matches_charge1'][i] == df_augmented['matches_charge2'][i] == df_augmented['matches_charge3'][i] == df_augmented['matches_charge4'][i] == df_augmented['matches_charge5'][i] == df_augmented['matches_charge6'][i]:\n",
    "        df_augmented.drop(i, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37ffbafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import another important function from tensorize script\n",
    "from tensorize import csv\n",
    "\n",
    "# to run csv function 'charge' needs to be replaced with 'precursor_charge' ?\n",
    "df_augmented.rename(columns = {'charge':'precursor_charge'}, inplace = True)\n",
    "\n",
    "# create dictionary of data\n",
    "data = csv(df_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4820e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import another important function from io_local script\n",
    "from io_local import to_hdf5, from_hdf5\n",
    "\n",
    "# convert dictionary data to hdf5 \n",
    "to_hdf5(data, 'hdf5_data.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45c70322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\miar\\Anaconda3\\envs\\py3.7env2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\miar\\Anaconda3\\envs\\py3.7env2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\miar\\Anaconda3\\envs\\py3.7env2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\miar\\Anaconda3\\envs\\py3.7env2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\miar\\Anaconda3\\envs\\py3.7env2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\miar\\Anaconda3\\envs\\py3.7env2.0\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\miar\\Anaconda3\\envs\\py3.7env2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\miar\\Anaconda3\\envs\\py3.7env2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\miar\\Anaconda3\\envs\\py3.7env2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\miar\\Anaconda3\\envs\\py3.7env2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\miar\\Anaconda3\\envs\\py3.7env2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\miar\\Anaconda3\\envs\\py3.7env2.0\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# load in ML packages and created data matrix\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras.utils import HDF5Matrix\n",
    "\n",
    "# load in hdf5 snd create matrix\n",
    "tensor = from_hdf5('hdf5_data.hdf5', n_samples=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e051f09f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18772\\2964060381.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmodel_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# load in model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\miar\\Downloads\\model_fragmentation_prediction\\prosit1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\DeNovo\\model.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(model_dir, trained)\u001b[0m\n\u001b[0;32m     43\u001b[0m         )\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtrained\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mweights_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py3.7env2.0\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py3.7env2.0\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[0;32m   1228\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[1;32m-> 1230\u001b[1;33m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[0;32m   1231\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'close'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py3.7env2.0\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, layers, reshape)\u001b[0m\n\u001b[0;32m   1181\u001b[0m     \"\"\"\n\u001b[0;32m   1182\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'keras_version'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m         \u001b[0moriginal_keras_version\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keras_version'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         \u001b[0moriginal_keras_version\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'1'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "# import model \n",
    "import training\n",
    "import model as model_lib\n",
    "# load in model\n",
    "model, model_config = model_lib.load(r\"C:\\Users\\miar\\Downloads\\model_fragmentation_prediction\\prosit1\", trained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf551a-c01e-4d60-8474-f14ff9d615c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import importlib\n",
    "#importlib.reload(model_lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c428b31a-ad3e-4ac1-8284-d1ac22de7bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44be6df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d39e9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d1f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd01b89f-68c4-4e51-a3e5-f6ef2e892437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3311a9e2-842c-43ce-afdf-76e06fad5218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14801731912183092702\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79c72271-0ac7-4ba9-bc06-d496f5bc5659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79c90ab0-7a34-401c-b6d7-d1ad6d0ae5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "print(get_available_devices()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210fc067-2279-49e6-826d-740bc0de70e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
